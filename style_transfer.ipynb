{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "This notebook runs through Section 2.3: Style Transfer. \n",
    "The images generated are similar to those in Figure 3. \n",
    "\n",
    "The Directory Paths block should be edited to suit local directory structure. \n",
    "The Chosen Parameters block can be changed to try out different experimental settings. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from utils.imaging import display_image, format_image, save_image\n",
    "from utils.losses import gram_matrix, style_layer_loss\n",
    "import vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to images we are extracting content and style from\n",
    "content_image_path = './images/perth_skyline.jpg'\n",
    "style_image_path = './images/starry_night.jpg'\n",
    "\n",
    "# Path to vgg19 checkpoint, must be downloaded separately\n",
    "checkpoint_path = './vgg_19.ckpt'\n",
    "\n",
    "# Location of tensorboard summaries\n",
    "tensorboard_dir = './train/'\n",
    "\n",
    "# Path to directory used for storing images\n",
    "debug_dir = './debug/'\n",
    "\n",
    "# Determines whether information is saved between runs\n",
    "# for tensorboard\n",
    "reset_saves = True\n",
    "if reset_saves is True:\n",
    "    # Ensure tensorboard is not running when you try to delete\n",
    "    # this directory\n",
    "    if os.path.exists(tensorboard_dir):\n",
    "        shutil.rmtree(tensorboard_dir)\n",
    "        \n",
    "# Create the debug directory if it doesn't exist\n",
    "# Tensorboard directory is made automatically if it doesn't exist\n",
    "if os.path.exists(debug_dir):\n",
    "    shutil.rmtree(debug_dir)\n",
    "os.makedirs(debug_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensions desired for image, channels must be kept as 3\n",
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "\n",
    "# Set the seeds to provide consistency between runs\n",
    "# Can also comment out for variability between runs\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# Layer being for content component\n",
    "content_layer = 'vgg_19/conv4/conv4_2'\n",
    "\n",
    "style_list = ['vgg_19/conv1/conv1_1', 'vgg_19/conv2/conv2_1', 'vgg_19/conv3/conv3_1',\n",
    "    'vgg_19/conv4/conv4_1', 'vgg_19/conv5/conv5_1']\n",
    "\n",
    "# Chosen depth corresponds to how many feature layers you want to use\n",
    "# for the style component\n",
    "chosen_depth = 5\n",
    "\n",
    "# Weights for each loss component\n",
    "content_weight = 1.0\n",
    "style_weight = content_weight/8e-4\n",
    "\n",
    "# Learning rate for optimizer\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Number of training and validation step\n",
    "# In this instance, validation refers to when we would like to examine:\n",
    "# currently optimized image and loss\n",
    "training_steps = 5000000\n",
    "validation_steps = 100000\n",
    "\n",
    "# Online debugging refers to images that will be displayed within the notebook \n",
    "# using plt, every validation step\n",
    "# Offline debugging refers to images that will be saved to folder using plt, \n",
    "# every validation step\n",
    "debug_online = True\n",
    "debug_offline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the input node and feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input node to the graph\n",
    "# These values are what is required by vgg19 for height, width, channels\n",
    "input_var_initial_value = np.random.rand(1, height, width, channels)\n",
    "input_var = tf.Variable(input_var_initial_value, dtype=tf.float32, name='input_var')\n",
    "\n",
    "# Load the vgg model\n",
    "with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "    end_points = vgg.vgg_19_conv(input_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up saver for vgg_19 variables only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare to restore the vgg19 nodes\n",
    "# Skip trying to restore the input variable since it's new\n",
    "all_variables = tf.get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list=all_variables[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load real images and create to-be-optimized image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the content image tensor\n",
    "# And the graph operation which assigns it to input_var\n",
    "content_image = plt.imread(content_image_path)\n",
    "content_image = cv2.resize(content_image, (height, width))\n",
    "content_image_batch = np.expand_dims(content_image, axis=0)\n",
    "content_image_batch = np.asarray(content_image_batch, dtype=np.float32)\n",
    "content_image_tensor = tf.Variable(content_image_batch, dtype=tf.float32, name='content_image')\n",
    "\n",
    "assign_content_image = tf.assign(input_var, content_image_tensor, name='assign_content_image')\n",
    "\n",
    "# Construct the style image tensor\n",
    "# And the graph operation which assigns it to input_var\n",
    "style_image = plt.imread(style_image_path)\n",
    "style_image = cv2.resize(style_image, (height, width))\n",
    "style_image_batch = np.expand_dims(style_image, axis=0)\n",
    "style_image_batch = np.asarray(style_image_batch, dtype=np.float32)\n",
    "style_image_tensor = tf.Variable(style_image_batch, dtype=tf.float32, name='style_image')\n",
    "\n",
    "assign_style_image = tf.assign(input_var, style_image_tensor, name='assign_style_image')\n",
    "\n",
    "# Construct the white noise tensor\n",
    "# And the graph operation which assigns it to input_var\n",
    "white_noise = np.random.rand(height, width, channels) * 255.\n",
    "white_noise_batch = np.expand_dims(white_noise, axis=0)\n",
    "white_noise_batch = np.asarray(white_noise_batch, dtype=np.float32)\n",
    "white_noise_tensor = tf.Variable(white_noise_batch, dtype=tf.float32, name='white_noise')\n",
    "\n",
    "assign_white_noise = tf.assign(input_var, white_noise_tensor, name='assign_white_noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up content extraction node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_rep = end_points[content_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain content representation for real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(),\n",
    "                  name='initialize_all')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize new variables and then restore vgg_19 variables\n",
    "    sess.run(init_op)\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    assign_content_image.eval()\n",
    "    \n",
    "    real_content_rep_ = sess.run(content_rep)\n",
    "    \n",
    "real_content_rep = tf.constant(real_content_rep_, name='real_content_rep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_loss = tf.losses.mean_squared_error(labels=real_content_rep, predictions=content_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up gram matrix nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grams = []\n",
    "filter_sizes = []\n",
    "for i in range(chosen_depth):\n",
    "    chosen_layer = end_points[style_list[i]]\n",
    "    gram_features = gram_matrix(chosen_layer)\n",
    "    grams.append(gram_features)\n",
    "    \n",
    "    # Determine the size of the filters used at each layer\n",
    "    # This is needed to calculate the loss from that layer\n",
    "    _, filter_height, filter_width, _ = chosen_layer.get_shape().as_list()\n",
    "    filter_size = float(filter_height * filter_width)\n",
    "    filter_sizes.append(filter_size)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain gram matrices for real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(),\n",
    "                  name='initialize_all')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize new variables and then restore vgg_19 variables\n",
    "    sess.run(init_op)\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    assign_style_image.eval()\n",
    "    \n",
    "    real_image_grams = sess.run(grams)\n",
    "    \n",
    "# Create constants with the real image gram matrices\n",
    "gram_constants = []\n",
    "for i in range(chosen_depth):\n",
    "    node_name = 'gram_constant_{}'.format(i)\n",
    "    gram_constant = tf.constant(real_image_grams[i], name=node_name)\n",
    "    gram_constants.append(gram_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_losses = []\n",
    "for i in range(chosen_depth):\n",
    "    layer_loss = style_layer_loss(gram_constants[i], grams[i], filter_sizes[i])\n",
    "    # Equal weighting on each loss, summing to 1\n",
    "    layer_loss *= (1.0 / chosen_depth)\n",
    "    layer_losses.append(layer_loss)\n",
    "style_loss = tf.add_n(layer_losses, name='sum_layer_losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the final loss, optimizer, and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = (content_weight * content_loss) + (style_weight * style_loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss, var_list=[input_var])\n",
    "\n",
    "# Initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(),\n",
    "                  name='initialize_all')\n",
    "\n",
    "# Tensorboard summaries\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "image_summary = tf.summary.image('image', input_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess: \n",
    "    # Initialize all variables and then\n",
    "    # restore weights for feature extractor\n",
    "    sess.run(init_op)\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    # Set up summary writer for tensorboard, saving graph as well\n",
    "    train_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n",
    "    \n",
    "    # Assign the white noise to the variable being optimized\n",
    "    assign_white_noise.eval()\n",
    "    \n",
    "    # Begin training\n",
    "    for i in range(training_steps):\n",
    "        loss_summary_, _ = sess.run([loss_summary, train_op])\n",
    "        train_writer.add_summary(loss_summary_, i)\n",
    "        \n",
    "        if i % validation_steps == 0:\n",
    "            image_summary_, current_image, loss_ = sess.run(\n",
    "                [image_summary, input_var, loss])\n",
    "            \n",
    "            train_writer.add_summary(image_summary_, i)\n",
    "            \n",
    "            print('Step: {}, Loss: {}'.format(i, loss_))\n",
    "            \n",
    "            if debug_online is True:\n",
    "                display_image(current_image, i)\n",
    "            if debug_offline is True:\n",
    "                save_image(current_image, i, debug_dir)  \n",
    "                \n",
    "    # Display and save the final image\n",
    "    current_image = input_var.eval()\n",
    "    display_image(current_image, 'Final')\n",
    "    save_image(current_image, 'Final', debug_dir)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
